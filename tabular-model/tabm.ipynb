{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepbiolab/data-science-hub/blob/main/tabular-model/tabm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aiegtaRzmdw"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# TabM\n",
        "\n",
        "This notebook provides a usage example of the `tabm` package from the\n",
        "[TabM](https://github.com/yandex-research/tabm) project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uEunLzUuzmdx",
        "outputId": "c5b3fd5a-ef60-468e-8625-c2e79688aa8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rtdl_num_embeddings\n",
            "  Downloading rtdl_num_embeddings-0.0.12-py3-none-any.whl.metadata (903 bytes)\n",
            "Requirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.11/dist-packages (from rtdl_num_embeddings) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=1.12->rtdl_num_embeddings)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.12->rtdl_num_embeddings) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.12->rtdl_num_embeddings) (3.0.2)\n",
            "Downloading rtdl_num_embeddings-0.0.12-py3-none-any.whl (13 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rtdl_num_embeddings\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rtdl_num_embeddings-0.0.12\n",
            "Collecting tabm\n",
            "  Downloading tabm-0.0.1-py3-none-any.whl.metadata (936 bytes)\n",
            "Requirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.11/dist-packages (from tabm) (2.6.0+cu124)\n",
            "Requirement already satisfied: rtdl_num_embeddings<0.1,>=0.0.12 in /usr/local/lib/python3.11/dist-packages (from tabm) (0.0.12)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from tabm) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->tabm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.12->tabm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.12->tabm) (3.0.2)\n",
            "Downloading tabm-0.0.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: tabm\n",
            "Successfully installed tabm-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rtdl_num_embeddings\n",
        "!pip install tabm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zb-mZr79zmdy"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from typing import Any, Literal, NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import tabm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZJ9sZ_JZzmdy"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "torch.manual_seed(seed + 2)\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227s3_WNzmdy"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn.datasets."
      ],
      "metadata": {
        "id": "O6KdY8k01B_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yd_f1PLGzmdy",
        "outputId": "db2c7b08-180a-4cbf-8948-d888dff05943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train    x_num    (323, 13)     float32\n",
            "train    y        (323,)        float32\n",
            "val      x_num    (81, 13)      float32\n",
            "val      y        (81,)         float32\n",
            "test     x_num    (102, 13)     float32\n",
            "test     y        (102,)        float32\n"
          ]
        }
      ],
      "source": [
        "# >>> Dataset.\n",
        "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
        "\n",
        "# Regression.\n",
        "task_type: TaskType = 'regression'\n",
        "n_classes = None\n",
        "\n",
        "# dataset = sklearn.datasets.fetch_california_housing()\n",
        "# X_num: np.ndarray = dataset['data']\n",
        "# Y: np.ndarray = dataset['target']\n",
        "\n",
        "dataset = sklearn.datasets.fetch_openml(data_id=531, as_frame=False)\n",
        "X_num: np.ndarray = dataset.data\n",
        "Y: np.ndarray = dataset.target\n",
        "\n",
        "\n",
        "# Classification.\n",
        "# n_classes = 2\n",
        "# assert n_classes >= 2\n",
        "# task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
        "# x_num, Y = sklearn.datasets.make_classification(\n",
        "#     n_samples=20000,\n",
        "#     n_features=8,\n",
        "#     n_classes=n_classes,\n",
        "#     n_informative=3,\n",
        "#     n_redundant=2,\n",
        "# )\n",
        "\n",
        "task_is_regression = task_type == 'regression'\n",
        "\n",
        "# >>> Numerical (continuous) features.\n",
        "X_num: np.ndarray = X_num.astype(np.float32)\n",
        "n_num_features = X_num.shape[1]\n",
        "\n",
        "# >>> Categorical features.\n",
        "# NOTE: the above datasets do not have categorical features, however,\n",
        "# for the demonstration purposes, it is possible to generate them.\n",
        "cat_cardinalities = [\n",
        "    # NOTE: uncomment the two lines below to add two categorical features.\n",
        "    # 4,  # Allowed values: [0, 1, 2, 3].\n",
        "    # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
        "]\n",
        "X_cat = (\n",
        "    np.column_stack([np.random.randint(0, c, (len(X_num),)) for c in cat_cardinalities])\n",
        "    if cat_cardinalities\n",
        "    else None\n",
        ")\n",
        "\n",
        "# >>> Labels.\n",
        "if task_type == 'regression':\n",
        "    Y = Y.astype(np.float32)\n",
        "else:\n",
        "    assert n_classes is not None\n",
        "    Y = Y.astype(np.int64)\n",
        "    assert set(Y.tolist()) == set(range(n_classes)), (\n",
        "        'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
        "    )\n",
        "\n",
        "# >>> Split the dataset.\n",
        "all_idx = np.arange(len(Y))\n",
        "trainval_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    all_idx, train_size=0.8\n",
        ")\n",
        "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
        "    trainval_idx, train_size=0.8\n",
        ")\n",
        "data_numpy = {\n",
        "    'train': {'x_num': X_num[train_idx], 'y': Y[train_idx]},\n",
        "    'val': {'x_num': X_num[val_idx], 'y': Y[val_idx]},\n",
        "    'test': {'x_num': X_num[test_idx], 'y': Y[test_idx]},\n",
        "}\n",
        "if X_cat is not None:\n",
        "    data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
        "    data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
        "    data_numpy['test']['x_cat'] = X_cat[test_idx]\n",
        "\n",
        "for part, part_data in data_numpy.items():\n",
        "    for key, value in part_data.items():\n",
        "        print(f'{part:<5}    {key:<5}    {value.shape!r:<10}    {value.dtype}')\n",
        "        del key, value\n",
        "    del part, part_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIEocbOVzmdz"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pe2PRlF3zmdz"
      },
      "outputs": [],
      "source": [
        "# Feature preprocessing.\n",
        "# NOTE\n",
        "# The choice between preprocessing strategies depends on a task and a model.\n",
        "\n",
        "# Simple preprocessing strategy.\n",
        "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
        "#     data_numpy['train']['x_num']\n",
        "# )\n",
        "\n",
        "# Advanced preprocessing strategy.\n",
        "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
        "x_num_train_numpy = data_numpy['train']['x_num']\n",
        "noise = (\n",
        "    np.random.default_rng(0)\n",
        "    .normal(0.0, 1e-5, x_num_train_numpy.shape)\n",
        "    .astype(x_num_train_numpy.dtype)\n",
        ")\n",
        "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
        "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
        "    output_distribution='normal',\n",
        "    subsample=10**9,\n",
        ").fit(x_num_train_numpy + noise)\n",
        "del x_num_train_numpy\n",
        "\n",
        "# Apply the preprocessing.\n",
        "for part in data_numpy:\n",
        "    data_numpy[part]['x_num'] = preprocessing.transform(data_numpy[part]['x_num'])\n",
        "\n",
        "\n",
        "# Label preprocessing.\n",
        "class RegressionLabelStats(NamedTuple):\n",
        "    mean: float\n",
        "    std: float\n",
        "\n",
        "\n",
        "Y_train = data_numpy['train']['y'].copy()\n",
        "if task_type == 'regression':\n",
        "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
        "    regression_label_stats = RegressionLabelStats(\n",
        "        Y_train.mean().item(), Y_train.std().item()\n",
        "    )\n",
        "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
        "else:\n",
        "    regression_label_stats = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xm4OS5Gzmdz"
      },
      "source": [
        "#  PyTorch settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tAtq7D3czmdz",
        "outputId": "9fa232ed-d07c-47b7-8350-9a534cf547bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:        CPU\n",
            "AMP:           False\n",
            "torch.compile: False\n"
          ]
        }
      ],
      "source": [
        "# Device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert data to tensors\n",
        "data = {\n",
        "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
        "    for part in data_numpy\n",
        "}\n",
        "Y_train = torch.as_tensor(Y_train, device=device)\n",
        "if task_type == 'regression':\n",
        "    for part in data:\n",
        "        data[part]['y'] = data[part]['y'].float()\n",
        "    Y_train = Y_train.float()\n",
        "\n",
        "# Automatic mixed precision (AMP)\n",
        "# torch.float16 is implemented for completeness,\n",
        "# but it was not tested in the project,\n",
        "# so torch.bfloat16 is used by default.\n",
        "amp_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        "    if torch.cuda.is_available()\n",
        "    else None\n",
        ")\n",
        "# Changing False to True can speed up training\n",
        "# of large enough models on compatible hardware.\n",
        "amp_enabled = False and amp_dtype is not None\n",
        "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
        "\n",
        "# torch.compile\n",
        "compile_model = False\n",
        "\n",
        "# fmt: off\n",
        "print(f'Device:        {device.type.upper()}')\n",
        "print(f'AMP:           {amp_enabled}{f\" ({amp_dtype})\"if amp_enabled else \"\"}')\n",
        "print(f'torch.compile: {compile_model}')\n",
        "# fmt: on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9NNKPtzmdz"
      },
      "source": [
        "# Model\n",
        "\n",
        "The best performance is usually achieved with `num_embeddings`\n",
        "from the `rtdl_num_embeddings` package. Typically, `PiecewiseLinearEmbeddings`\n",
        "and `PeriodicEmbeddings` perform best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PrbhXzG9zmdz",
        "outputId": "2f8d41ce-5db0-4a88-e7d1-a8d6f00e6607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/rtdl_num_embeddings.py:340: UserWarning: The 3-th feature has just two bin edges, which means only one bin. Strictly speaking, using a single bin for the piecewise-linear encoding should not break anything, but it is the same as using sklearn.preprocessing.MinMaxScaler\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# No embeddings.\n",
        "num_embeddings = None\n",
        "\n",
        "# Simple embeddings.\n",
        "num_embeddings = rtdl_num_embeddings.LinearReLUEmbeddings(n_num_features)\n",
        "\n",
        "# Periodic embeddings.\n",
        "num_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(n_num_features, lite=False)\n",
        "\n",
        "# Piecewise-linear embeddings.\n",
        "num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
        "    rtdl_num_embeddings.compute_bins(data['train']['x_num'], n_bins=48),\n",
        "    d_embedding=16,\n",
        "    activation=False,\n",
        "    version='B',\n",
        ")\n",
        "\n",
        "model = tabm.TabM.make(\n",
        "    n_num_features=n_num_features,\n",
        "    cat_cardinalities=cat_cardinalities,\n",
        "    d_out=1 if n_classes is None else n_classes,\n",
        "    num_embeddings=num_embeddings,\n",
        ").to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=3e-4)\n",
        "\n",
        "if compile_model:\n",
        "    # NOTE\n",
        "    # `torch.compile(model, mode=\"reduce-overhead\")` caused issues during training,\n",
        "    # so the `mode` argument is not used.\n",
        "    model = torch.compile(model)\n",
        "    evaluation_mode = torch.no_grad\n",
        "else:\n",
        "    evaluation_mode = torch.inference_mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCB2jrczzmdz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1DfgHVZwzmd0"
      },
      "outputs": [],
      "source": [
        "# A quick reminder: TabM represents an ensemble of k MLPs.\n",
        "#\n",
        "# The option below determines if the MLPs are trained\n",
        "# on the same batches (share_training_batches=True) or\n",
        "# on different batches. Technically, this option determines:\n",
        "# - How the loss function is implemented.\n",
        "# - How the training batches are constructed.\n",
        "#\n",
        "# `True` is recommended by default because of better training efficiency.\n",
        "# On some tasks, `False` may provide better performance.\n",
        "share_training_batches = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aHaftUqyzmd0",
        "outputId": "498a8d51-b891-473c-cb3e-1a40c2da57cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score before training: -9.5777\n"
          ]
        }
      ],
      "source": [
        "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
        "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
        "    return (\n",
        "        model(\n",
        "            data[part]['x_num'][idx],\n",
        "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
        "        )\n",
        "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
        "        .float()\n",
        "    )\n",
        "\n",
        "\n",
        "base_loss_fn = F.mse_loss if task_is_regression else F.cross_entropy\n",
        "\n",
        "\n",
        "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
        "    # TabM produces k predictions. Each of them must be trained separately.\n",
        "\n",
        "    # Regression:     (batch_size, k)            -> (batch_size * k,)\n",
        "    # Classification: (batch_size, k, n_classes) -> (batch_size * k, n_classes)\n",
        "    y_pred = y_pred.flatten(0, 1)\n",
        "\n",
        "    if share_training_batches:\n",
        "        # (batch_size,) -> (batch_size * k,)\n",
        "        y_true = y_true.repeat_interleave(model.backbone.k)\n",
        "    else:\n",
        "        # (batch_size, k) -> (batch_size * k,)\n",
        "        y_true = y_true.flatten(0, 1)\n",
        "\n",
        "    return base_loss_fn(y_pred, y_true)\n",
        "\n",
        "\n",
        "@evaluation_mode()\n",
        "def evaluate(part: str) -> float:\n",
        "    model.eval()\n",
        "\n",
        "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
        "    eval_batch_size = 8096\n",
        "    y_pred: np.ndarray = (\n",
        "        torch.cat(\n",
        "            [\n",
        "                apply_model(part, idx)\n",
        "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
        "                    eval_batch_size\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "    if task_type == 'regression':\n",
        "        # Transform the predictions back to the original label space.\n",
        "        assert regression_label_stats is not None\n",
        "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
        "\n",
        "    # Compute the mean of the k predictions.\n",
        "    if not task_is_regression:\n",
        "        # For classification, the mean must be computed in the probability space.\n",
        "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
        "    y_pred = y_pred.mean(1)\n",
        "\n",
        "    y_true = data[part]['y'].cpu().numpy()\n",
        "    score = (\n",
        "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
        "        if task_type == 'regression'\n",
        "        else sklearn.metrics.accuracy_score(y_true, y_pred.argmax(1))\n",
        "    )\n",
        "    return float(score)  # The higher -- the better.\n",
        "\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DpTQFWLzzmd0",
        "outputId": "77ab7d00-36a0-4225-bfb7-da3febd8cfa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [epoch] 0   [val] -8.675 [test] -8.609\n",
            "* [epoch] 1   [val] -7.304 [test] -7.159\n",
            "* [epoch] 2   [val] -6.013 [test] -6.058\n",
            "* [epoch] 3   [val] -5.525 [test] -5.680\n",
            "* [epoch] 4   [val] -5.102 [test] -5.051\n",
            "  [epoch] 5   [val] -5.170 [test] -4.905\n",
            "  [epoch] 6   [val] -5.442 [test] -4.926\n",
            "* [epoch] 7   [val] -5.011 [test] -4.390\n",
            "* [epoch] 8   [val] -4.932 [test] -4.218\n",
            "  [epoch] 9   [val] -4.977 [test] -4.243\n",
            "* [epoch] 10  [val] -4.883 [test] -4.181\n",
            "* [epoch] 11  [val] -4.781 [test] -4.046\n",
            "  [epoch] 12  [val] -5.149 [test] -4.230\n",
            "  [epoch] 13  [val] -4.887 [test] -3.954\n",
            "  [epoch] 14  [val] -4.794 [test] -3.877\n",
            "  [epoch] 15  [val] -4.938 [test] -3.930\n",
            "  [epoch] 16  [val] -4.789 [test] -3.777\n",
            "  [epoch] 17  [val] -4.800 [test] -3.752\n",
            "  [epoch] 18  [val] -4.895 [test] -3.716\n",
            "* [epoch] 19  [val] -4.729 [test] -3.569\n",
            "* [epoch] 20  [val] -4.597 [test] -3.525\n",
            "  [epoch] 21  [val] -4.711 [test] -3.575\n",
            "  [epoch] 22  [val] -4.748 [test] -3.598\n",
            "* [epoch] 23  [val] -4.531 [test] -3.517\n",
            "  [epoch] 24  [val] -4.532 [test] -3.428\n",
            "  [epoch] 25  [val] -4.768 [test] -3.468\n",
            "  [epoch] 26  [val] -4.580 [test] -3.286\n",
            "  [epoch] 27  [val] -4.550 [test] -3.254\n",
            "  [epoch] 28  [val] -4.779 [test] -3.545\n",
            "* [epoch] 29  [val] -4.347 [test] -3.309\n",
            "* [epoch] 30  [val] -4.293 [test] -3.355\n",
            "  [epoch] 31  [val] -4.407 [test] -3.432\n",
            "  [epoch] 32  [val] -4.340 [test] -3.272\n",
            "  [epoch] 33  [val] -4.368 [test] -3.162\n",
            "  [epoch] 34  [val] -4.341 [test] -3.117\n",
            "  [epoch] 35  [val] -4.325 [test] -3.092\n",
            "* [epoch] 36  [val] -4.281 [test] -3.052\n",
            "  [epoch] 37  [val] -4.343 [test] -3.142\n",
            "* [epoch] 38  [val] -4.253 [test] -3.067\n",
            "* [epoch] 39  [val] -4.248 [test] -3.034\n",
            "  [epoch] 40  [val] -4.291 [test] -3.022\n",
            "* [epoch] 41  [val] -4.224 [test] -3.018\n",
            "  [epoch] 42  [val] -4.258 [test] -2.957\n",
            "  [epoch] 43  [val] -4.233 [test] -2.897\n",
            "  [epoch] 44  [val] -4.385 [test] -3.042\n",
            "* [epoch] 45  [val] -4.101 [test] -2.887\n",
            "* [epoch] 46  [val] -4.064 [test] -2.894\n",
            "  [epoch] 47  [val] -4.146 [test] -2.896\n",
            "  [epoch] 48  [val] -4.131 [test] -2.821\n",
            "  [epoch] 49  [val] -4.261 [test] -2.942\n",
            "  [epoch] 50  [val] -4.076 [test] -2.798\n",
            "  [epoch] 51  [val] -4.140 [test] -2.788\n",
            "  [epoch] 52  [val] -4.136 [test] -2.846\n",
            "* [epoch] 53  [val] -3.966 [test] -2.862\n",
            "  [epoch] 54  [val] -4.138 [test] -2.908\n",
            "  [epoch] 55  [val] -4.074 [test] -2.793\n",
            "  [epoch] 56  [val] -3.981 [test] -2.801\n",
            "  [epoch] 57  [val] -4.357 [test] -3.291\n",
            "* [epoch] 58  [val] -3.899 [test] -2.713\n",
            "  [epoch] 59  [val] -3.943 [test] -2.666\n",
            "  [epoch] 60  [val] -4.205 [test] -2.943\n",
            "* [epoch] 61  [val] -3.890 [test] -2.773\n",
            "  [epoch] 62  [val] -3.958 [test] -2.678\n",
            "  [epoch] 63  [val] -4.138 [test] -2.787\n",
            "* [epoch] 64  [val] -3.869 [test] -2.740\n",
            "* [epoch] 65  [val] -3.858 [test] -2.752\n",
            "  [epoch] 66  [val] -3.963 [test] -2.744\n",
            "  [epoch] 67  [val] -3.901 [test] -2.637\n",
            "  [epoch] 68  [val] -3.898 [test] -2.650\n",
            "  [epoch] 69  [val] -3.929 [test] -2.600\n",
            "  [epoch] 70  [val] -3.895 [test] -2.589\n",
            "* [epoch] 71  [val] -3.772 [test] -2.622\n",
            "  [epoch] 72  [val] -3.841 [test] -2.757\n",
            "  [epoch] 73  [val] -3.924 [test] -2.644\n",
            "  [epoch] 74  [val] -3.964 [test] -2.550\n",
            "  [epoch] 75  [val] -3.888 [test] -2.607\n",
            "  [epoch] 76  [val] -3.813 [test] -2.723\n",
            "  [epoch] 77  [val] -3.882 [test] -2.600\n",
            "  [epoch] 78  [val] -3.928 [test] -2.606\n",
            "  [epoch] 79  [val] -3.865 [test] -2.671\n",
            "  [epoch] 80  [val] -3.926 [test] -2.673\n",
            "  [epoch] 81  [val] -3.860 [test] -2.563\n",
            "  [epoch] 82  [val] -3.808 [test] -2.572\n",
            "  [epoch] 83  [val] -3.839 [test] -2.664\n",
            "  [epoch] 84  [val] -3.793 [test] -2.570\n",
            "  [epoch] 85  [val] -3.805 [test] -2.524\n",
            "  [epoch] 86  [val] -3.837 [test] -2.534\n",
            "  [epoch] 87  [val] -3.809 [test] -2.544\n",
            "  [epoch] 88  [val] -3.773 [test] -2.503\n",
            "\n",
            "[Summary]\n",
            "best epoch:  71\n",
            "val score:  -3.7724711758452187\n",
            "test score: -2.6222082460519496\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 1_000_000_000\n",
        "train_size = len(train_idx)\n",
        "batch_size = 256\n",
        "epoch_size = math.ceil(train_size / batch_size)\n",
        "\n",
        "epoch = -1\n",
        "metrics = {'val': -math.inf, 'test': -math.inf}\n",
        "\n",
        "\n",
        "def make_checkpoint() -> dict[str, Any]:\n",
        "    return deepcopy(\n",
        "        {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'metrics': metrics,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "best_checkpoint = make_checkpoint()\n",
        "\n",
        "# Early stopping: the training stops if the validation score\n",
        "# does not improve for more than `patience` consecutive epochs.\n",
        "patience = 16\n",
        "remaining_patience = patience\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    batches = (\n",
        "        # Create one standard batch sequence.\n",
        "        torch.randperm(train_size, device=device).split(batch_size)\n",
        "        if share_training_batches\n",
        "        # Create k independent batch sequences.\n",
        "        else (\n",
        "            torch.rand((train_size, model.backbone.k), device=device)\n",
        "            .argsort(dim=0)\n",
        "            .split(batch_size, dim=0)\n",
        "        )\n",
        "    )\n",
        "    for batch_idx in batches:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
        "        if grad_scaler is None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            grad_scaler.scale(loss).backward()  # type: ignore\n",
        "            grad_scaler.step(optimizer)\n",
        "            grad_scaler.update()\n",
        "\n",
        "    metrics = {part: evaluate(part) for part in ['val', 'test']}\n",
        "    val_score_improved = metrics['val'] > best_checkpoint['metrics']['val']\n",
        "\n",
        "    print(\n",
        "        f'{\"*\" if val_score_improved else \" \"}'\n",
        "        f' [epoch] {epoch:<3}'\n",
        "        f' [val] {metrics[\"val\"]:.3f}'\n",
        "        f' [test] {metrics[\"test\"]:.3f}'\n",
        "    )\n",
        "\n",
        "    if val_score_improved:\n",
        "        best_checkpoint = make_checkpoint()\n",
        "        remaining_patience = patience\n",
        "    else:\n",
        "        remaining_patience -= 1\n",
        "\n",
        "    if remaining_patience < 0:\n",
        "        break\n",
        "\n",
        "# To make final predictions, load the best checkpoint.\n",
        "model.load_state_dict(best_checkpoint['model'])\n",
        "\n",
        "print('\\n[Summary]')\n",
        "print(f'best epoch:  {best_checkpoint[\"epoch\"]}')\n",
        "print(f'val score:  {best_checkpoint[\"metrics\"][\"val\"]}')\n",
        "print(f'test score: {best_checkpoint[\"metrics\"][\"test\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95FfQh5U1zlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}